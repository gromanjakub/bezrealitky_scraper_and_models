import requests
import pandas
import csv
import urllib.request

pocetstranek = 0 #pocet stranek na webu
stranky_list = []
seznam_parametru = []
### PRO STRÁNKU
llink = "https://www.bezrealitky.cz/vypis/nabidka-prodej/byt?page=" #k tomuhle budu přidávat čísla stránek

get_bezrealitky = requests.get(("https://www.bezrealitky.cz/vypis/nabidka-prodej/byt?page=1")) #zde získávám obsah 1.stránky
source_bezrealitky = get_bezrealitky.content
soup_bezrealitky = BeautifulSoup(source_bezrealitky, features="html.parser")
stranky_pomoc=[]
for link in soup_bezrealitky.find_all('a'): #získávám odkazy
    stranky_pomoc.append(link.get("href"))
    
for stranka in stranky_pomoc:
    if "page" in stranka:
        stranky_list.append(stranka)
pocetstranek = int(stranky_list[-2].split("=")[1])+1 #vezmu předposlední link na stránku (poslední je "next"), vezmu to za =, range funkce jde do x-1

for x in range(1,3): #misto cisla pak pocetstranek
    web = ("https://www.bezrealitky.cz/vypis/nabidka-prodej/byt?page=" + str(x))
    get_bezrealitky = requests.get(web) #zde získávám obsah 1.stránky
    source_bezrealitky = get_bezrealitky.content
    soup_bezrealitky = BeautifulSoup(source_bezrealitky, features="html.parser")
    
    
    
    stranky_pomoc=[]
    for link in soup_bezrealitky.find_all('a'): #získávám odkazy
        stranky_pomoc.append(link.get("href"))
    
    for stranka in stranky_pomoc:
        if "page" in stranka:
            stranky_list.append(stranka)
            
    


    hrefy = []
    for link in soup_bezrealitky.find_all('a'):
        hrefy.append(link.get("href"))
    
    odkazy = []    
    for href in hrefy:    
        if "nemovitosti-byty-domy" in href:
            odkazy.append(href)
            
   
    odkazy = list(set(odkazy)) #za každou fotku se tam objeví jeden link, setem udělám unique, dávám na list aby se s tím dalo pracovat
    
    link = "https://www.bezrealitky.cz" #base link pro odkazovani na inzeraty
    inzeraty = [(link + odkaz) for odkaz in odkazy] #tvoření linků na jednotlivé inzeráty
    
    ### PRO INZERÁTY 
    
    
    for inzerat in inzeraty:    
        get_inzerat = requests.get(inzerat)  #zde získám obsah 1. inzerátů na aktuální stránce
        source_inzerat = get_inzerat.content
        soup_inzerat = BeautifulSoup(source_inzerat, features="html.parser")
        
        inzerat_divy = soup_inzerat.find_all('div', attrs={"class":"col col-6 param-value"}) #zde vytahuju tabulku parametrů atd. z aktuálního inzerátu
        vnitrek_divu = [(i.contents[0].text) for i in inzerat_divy]
        
        parametry = []        
        for m in vnitrek_divu: #zde čistím parametry o /n
            m = m.strip()
            parametry.append(m)
        
      
        parametry.pop(0) #tím se zbavím prázdného co zbylo po reklamě na internet nebo co
        parametry.pop(-1)
        
        seznam_parametru.append(parametry)
        
        print("Jdu na " + inzerat)
                
with open("/home/kub/pocetstranek.csv", "w", newline="") as f:   #tohle pak oživím až bude třeba dát output do csv
    writer = csv.writer(f)
    writer.writerows(seznam_parametru)    

"""    
import pandas as pd
file_name = "my_file_with_dupes.csv"
file_name_output = "my_file_without_dupes.csv"

df = pd.read_csv(file_name, sep="\t or ,")            tohle využiju na zbavení se duplikátů v csv
df.drop_duplicates(subset=None, inplace=True)

# Write the results to a different file
df.to_csv(file_name_output, index=False)    
"""    


import urllib.request

inzerat_divy = soup_inzerat.find_all('div', attrs={"class":"detail-slick-item"}) #zde vytahuju tabulku parametrů atd. z aktuálního inzerátu
vnitrek_divu_obrazky = [(i.contents[0].text) for i in inzerat_divy]
inzerat_divy2 = soup_inzerat.find_all("img")

list_img = []
for img in inzerat_divy2:
    list_img.append(img["src"])
    
list_img = list(set(list_img)) #mažu duplikáty, dělám list abych s tím mohl dále pracovat


cisty_list_img = []
for item in list_img: #tento celý blok slouží k vyfiltrování jenom velkých fotek, bez thumbnailů a useless odkazů
    if "jpg" in item:
        if "https" in item:
            if "thumb" not in item: #z nějakého fascinujícího důvodu mi nefunguje kombinování více podmínek, dává mi to tam ty krátké odkazy
                cisty_list_img.append(item)  
                
ffr = list(range(len(cisty_list_img))) #pomocný list na tvoření jmen souborů co stáhnu
for obrazek in cisty_list_img: #tady stahuju jednotlivé obrázky
    cislo = cisty_list_img.index(obrazek)
    adresa = "/home/kub/fefe/" + str(ffr[cislo]) #neumí to vytvořit obrázek když tam je celý odkaz z obrázku, tak musím nejdřív udělat list jmen a ty pak použít 
    urllib.request.urlretrieve(obrazek, adresa)
    
